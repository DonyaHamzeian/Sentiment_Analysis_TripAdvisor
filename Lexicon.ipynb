{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code I have found on the internet to score a test based on sentiWordNet from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class to score sentiment of text.\n",
    "Use domain-independent method of dictionary lookup of sentiment words,\n",
    "handling negations and multiword expressions. Based on SentiWordNet 3.0.\n",
    "\"\"\"\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "\n",
    "class SentimentAnalysis(object):\n",
    "    \"\"\"Class to get sentiment score based on analyzer.\"\"\"\n",
    "\n",
    "    def __init__(self, filename='SentiWordNet.txt', weighting='geometric'):\n",
    "        \"\"\"Initialize with filename and choice of weighting.\"\"\"\n",
    "        if weighting not in ('geometric', 'harmonic', 'average'):\n",
    "            raise ValueError(\n",
    "                'Allowed weighting options are geometric, harmonic, average')\n",
    "        # parse file and build sentiwordnet dicts\n",
    "        self.swn_pos = {'a': {}, 'v': {}, 'r': {}, 'n': {}}\n",
    "        self.swn_all = {}\n",
    "        self.build_swn(filename, weighting)\n",
    "\n",
    "    def average(self, score_list):\n",
    "        \"\"\"Get arithmetic average of scores.\"\"\"\n",
    "        if(score_list):\n",
    "            return sum(score_list) / float(len(score_list))\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def geometric_weighted(self, score_list):\n",
    "        \"\"\"\"Get geometric weighted sum of scores.\"\"\"\n",
    "        weighted_sum = 0\n",
    "        num = 1\n",
    "        for el in score_list:\n",
    "            weighted_sum += (el * (1 / float(2**num)))\n",
    "            num += 1\n",
    "        return weighted_sum\n",
    "\n",
    "    # another possible weighting instead of average\n",
    "    def harmonic_weighted(self, score_list):\n",
    "        \"\"\"Get harmonic weighted sum of scores.\"\"\"\n",
    "        weighted_sum = 0\n",
    "        num = 2\n",
    "        for el in score_list:\n",
    "            weighted_sum += (el * (1 / float(num)))\n",
    "            num += 1\n",
    "        return weighted_sum\n",
    "\n",
    "    def build_swn(self, filename, weighting):\n",
    "        \"\"\"Build class's lookup based on SentiWordNet 3.0.\"\"\"\n",
    "        records = [line.split('\\t') for line in open(filename)]\n",
    "        for rec in records:\n",
    "            # has many words in 1 entry\n",
    "            words = rec[4].split()\n",
    "            pos = rec[0]\n",
    "            for word_num in words:\n",
    "                word = word_num.split('#')[0]\n",
    "                sense_num = int(word_num.split('#')[1])\n",
    "\n",
    "                # build a dictionary key'ed by sense number\n",
    "                if word not in self.swn_pos[pos]:\n",
    "                    self.swn_pos[pos][word] = {}\n",
    "                self.swn_pos[pos][word][sense_num] = float(\n",
    "                    rec[2]) - float(rec[3])\n",
    "                if word not in self.swn_all:\n",
    "                    self.swn_all[word] = {}\n",
    "                self.swn_all[word][sense_num] = float(rec[2]) - float(rec[3])\n",
    "\n",
    "        # convert innermost dicts to ordered lists of scores\n",
    "        for pos in self.swn_pos.keys():\n",
    "            for word in self.swn_pos[pos].keys():\n",
    "                newlist = [self.swn_pos[pos][word][k] for k in sorted(\n",
    "                    self.swn_pos[pos][word].keys())]\n",
    "                if weighting == 'average':\n",
    "                    self.swn_pos[pos][word] = self.average(newlist)\n",
    "                if weighting == 'geometric':\n",
    "                    self.swn_pos[pos][word] = self.geometric_weighted(newlist)\n",
    "                if weighting == 'harmonic':\n",
    "                    self.swn_pos[pos][word] = self.harmonic_weighted(newlist)\n",
    "\n",
    "        for word in self.swn_all.keys():\n",
    "            newlist = [self.swn_all[word][k] for k in sorted(\n",
    "                self.swn_all[word].keys())]\n",
    "            if weighting == 'average':\n",
    "                self.swn_all[word] = self.average(newlist)\n",
    "            if weighting == 'geometric':\n",
    "                self.swn_all[word] = self.geometric_weighted(newlist)\n",
    "            if weighting == 'harmonic':\n",
    "                self.swn_all[word] = self.harmonic_weighted(newlist)\n",
    "\n",
    "    def pos_short(self, pos):\n",
    "        \"\"\"Convert NLTK POS tags to SWN's POS tags.\"\"\"\n",
    "        if pos in set(['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']):\n",
    "            return 'v'\n",
    "        elif pos in set(['JJ', 'JJR', 'JJS']):\n",
    "            return 'a'\n",
    "        elif pos in set(['RB', 'RBR', 'RBS']):\n",
    "            return 'r'\n",
    "        elif pos in set(['NNS', 'NN', 'NNP', 'NNPS']):\n",
    "            return 'n'\n",
    "        else:\n",
    "            return 'a'\n",
    "\n",
    "    def score_word(self, word, pos):\n",
    "        \"\"\"Get sentiment score of word based on SWN and part of speech.\"\"\"\n",
    "        try:\n",
    "            return self.swn_pos[pos][word]\n",
    "        except KeyError:\n",
    "            try:\n",
    "                return self.swn_all[word]\n",
    "            except KeyError:\n",
    "                return 0\n",
    "\n",
    "    def score(self, sentence):\n",
    "        \"\"\"Sentiment score a sentence.\"\"\"\n",
    "        # init sentiwordnet lookup/scoring tools\n",
    "        impt = set(['NNS', 'NN', 'NNP', 'NNPS', 'JJ', 'JJR', 'JJS',\n",
    "                    'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN',\n",
    "                    'VBP', 'VBZ', 'unknown'])\n",
    "        non_base = set(['VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'NNS', 'NNPS'])\n",
    "        negations = set(['not', 'n\\'t', 'less', 'no', 'never',\n",
    "                         'nothing', 'nowhere', 'hardly', 'barely',\n",
    "                         'scarcely', 'nobody', 'none'])\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "        scores = []\n",
    "        tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "        index = 0\n",
    "        for el in tagged:\n",
    "\n",
    "            pos = el[1]\n",
    "            try:\n",
    "                word = re.match('(\\w+)', el[0]).group(0).lower()\n",
    "                start = index - 5\n",
    "                if start < 0:\n",
    "                    start = 0\n",
    "                neighborhood = tokens[start:index]\n",
    "\n",
    "                # look for trailing multiword expressions\n",
    "                word_minus_one = tokens[index-1:index+1]\n",
    "                word_minus_two = tokens[index-2:index+1]\n",
    "\n",
    "                # if multiword expression, fold to one expression\n",
    "                if(self.is_multiword(word_minus_two)):\n",
    "                    if len(scores) > 1:\n",
    "                        scores.pop()\n",
    "                        scores.pop()\n",
    "                    if len(neighborhood) > 1:\n",
    "                        neighborhood.pop()\n",
    "                        neighborhood.pop()\n",
    "                    word = '_'.join(word_minus_two)\n",
    "                    pos = 'unknown'\n",
    "\n",
    "                elif(self.is_multiword(word_minus_one)):\n",
    "                    if len(scores) > 0:\n",
    "                        scores.pop()\n",
    "                    if len(neighborhood) > 0:\n",
    "                        neighborhood.pop()\n",
    "                    word = '_'.join(word_minus_one)\n",
    "                    pos = 'unknown'\n",
    "\n",
    "                # perform lookup\n",
    "                if (pos in impt) and (word not in stopwords):\n",
    "                    if pos in non_base:\n",
    "                        word = wnl.lemmatize(word, self.pos_short(pos))\n",
    "                    score = self.score_word(word, self.pos_short(pos))\n",
    "                    if len(negations.intersection(set(neighborhood))) > 0:\n",
    "                        score = -score\n",
    "                    scores.append(score)\n",
    "\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "            index += 1\n",
    "\n",
    "        if len(scores) > 0:\n",
    "            return sum(scores) / float(len(scores))\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def is_multiword(self, words):\n",
    "        \"\"\"Test if a group of words is a multiword expression.\"\"\"\n",
    "        joined = '_'.join(words)\n",
    "        return joined in self.swn_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://donya:@localhost:5432/tripadvisor')\n",
    "df = pd.read_sql('test_senti', engine)\n",
    "# df['SentiWordnetH'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the data has been read correctly from postgresql database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'wife', 'and', 'i', 'recently', 'flew', 'jet', 'airways', 'from', 'mumbai', 'to', 'lhr', 'and', 'lhr', 'to', 'delhi', 'and', 'flights', 'were', 'impeccable', 'for', 'one', 'we', 'were', 'ahead', 'of', 'schedule', 'both', 'legs', 'something', 'which', 'regular', 'travellers', 'value', 'most', 'check', 'in', 'and', 'boarding', 'were', 'efficient', 'and', 'without', 'fuss', '737', 'ers', 'were', 'configured', 'perfectly', 'whilst', 'we', 'flew', 'business', 'seats', 'in', 'economy']\n"
     ]
    }
   ],
   "source": [
    "text = df['preprocessed_text'][1]\n",
    "words = text.split()\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentiWordNet Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = SentimentAnalysis(filename='SentiWordNet.txt',weighting='harmonic')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Remove Stop words(a, an, the, punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Remove Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 3. Convert Negative to negtag_ ( “not”, “no”, “none”, “nei- ther”, “never” or “nobody”.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Convert Numbers (I have commented this part, I can do it later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Get Sentiment Score based on SentiWordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a', 'an', 'the'}\n"
     ]
    }
   ],
   "source": [
    "neg_pat = re.compile(r'( not | nobody | none | no | neither | never )(\\w+)')\n",
    "stopwords = set(['a', 'an', 'the'])\n",
    "print(stopwords)\n",
    "stop_pat = re.compile('(\\s)a(\\s)|(\\s)an(\\s)|(\\s)the(\\s)')\n",
    "df['preprocessed_text'] = df['text_review']\n",
    "df['preprocessed_text'] = df['preprocessed_text'].str.lower()\n",
    "#remove stopwords\n",
    "df['preprocessed_text'] = df['preprocessed_text'].apply(lambda x: stop_pat.sub(' ', x) )\n",
    "\n",
    "#remove punctuation\n",
    "df['preprocessed_text'] = df['preprocessed_text'].apply(lambda x: re.sub(r'[^\\w\\s]',' ',x))\n",
    "\n",
    "#convert Negative\n",
    "df['preprocessed_text'] = df['preprocessed_text'].apply(lambda x: re.sub(neg_pat.search(x).group(),' neg_'+ neg_pat.search(x).group(2) , x) if len(neg_pat.findall(x))>0 else x )\n",
    "\n",
    "#Convert Numbers\n",
    "# df.loc[i, 'text_review']=  re.sub(' [0-9]+ ', 'NumTag', review)\n",
    "\n",
    "df['SentiWordnetH']= df['preprocessed_text'].apply(lambda x: s.score(x) if x is not np.nan else -1000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Term Document Matrix(unigram & bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_bad = df['review_rating']<40\n",
    "is_bad = np.array(is_bad)\n",
    "is_good = df['review_rating']>=40\n",
    "is_good =np.array(is_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(604625, 2786495)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(ngram_range=(1, 2))\n",
    "X = vec.fit_transform(df['preprocessed_text'])\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 (Weighted field-specific lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_score = pd.DataFrame({'neg': np.ravel(np.sum(X[is_bad,:],axis=0)), 'pos': np.ravel(np.sum(X[is_good, :],axis=0))}, index = vec.get_feature_names())\n",
    "words_score['SW']= (words_score['pos']- words_score['neg'])/(words_score['pos']+ words_score['neg'])\n",
    "words_score_filtered =words_score.loc[words_score['pos']+words_score['neg'] >75]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25   -0.101636\n",
      "0.50    0.151918\n",
      "0.75    0.401264\n",
      "Name: SW, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(words_score_filtered.SW.quantile([0.25,0.5,0.75]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the first quartile 'Negative' and 4th quartile 'Positive' (As said in the article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_score_neg = words_score_filtered.loc[words_score_filtered['SW']<=-0.101636] \n",
    "words_score_pos = words_score_filtered.loc[words_score_filtered['SW']>= 0.401264] \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save SentiWordnet Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_score_neg.to_sql('Negative Words', engine)\n",
    "words_score_pos.to_sql('Positive Words', engine)\n",
    "words_score_filtered.to_sql('AllWordsScore', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Prediction from L1(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a dictionary\n",
    "lexicon_pd = pd.concat([words_score_neg['SW'], words_score_pos['SW']], axis=0)\n",
    "lexicon = lexicon_pd.to_dict()\n",
    "def calculateScore(text):\n",
    "    score = 0\n",
    "    count = 0\n",
    "    for word in text.split():\n",
    "        if word in lexicon:\n",
    "            score = score + lexicon[word]\n",
    "            count +=1\n",
    "    if count >0:\n",
    "        return score/count\n",
    "    else:\n",
    "        #the lexicon could not analyse this text\n",
    "        return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['L1_prediction']= df['preprocessed_text'].apply(lambda x: calculateScore(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lexicon could not assign polarity to 185 texts(train) and 78 texts(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['L1_prediction'][df['L1_prediction']==-1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following you can see these texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4123       plane itself did neg_impress me as it was pre...\n",
       "6201      \\nthought we were taking quality airline on re...\n",
       "6687      \\nmy friends flew with different company  had ...\n",
       "6802      \\nthere really isnt much you can review on ba ...\n",
       "9073      \\n我是用15 000 asia milies 換  個心態是不想浪漫15 000分  於是...\n",
       "10925     \\nairline is getting smaller and smaller  if t...\n",
       "14179      whole experience  checking in  boarding  cabi...\n",
       "27461     \\nrestrictions  restrictions  restrictions  an...\n",
       "29530     \\nmein eingecheckter koffer sowie der meiner f...\n",
       "30892     \\ni observed how much stewardess were sullen  ...\n",
       "32737     \\nshort flight to phitsanulok and seems ok for...\n",
       "34117     \\nthis was small plane so we had to check our ...\n",
       "38562     \\nsikerült belefutni egy őrült check in pultos...\n",
       "38718     \\nyou know what you re getting with ryanair  i...\n",
       "45464     \\nthis was lot more better experience than pre...\n",
       "49731     \\nde classificatie  bijzonder goed  krijgt tra...\n",
       "51912     \\nnothing special  but we had problem with spe...\n",
       "54314     \\n길이 막혀서 공항에 허둥지둥 도착하니 짐을 부칠 수 있는 라벨은 곳 곳에 세워둔...\n",
       "55681     \\nflew from ottawa to st  louis via washington...\n",
       "62961      plane was empty  i could have had entire row ...\n",
       "64004      morning flight from ord to lax is near flagsh...\n",
       "67453     \\nif going to columbus  ohio aa direct flight ...\n",
       "68424     \\ncompared to  cheap  airlines  number of flig...\n",
       "70633     \\ni use to fly with other airlines to yerevan ...\n",
       "70881     \\nflight av 316   av 671 of avianca from panam...\n",
       "71386     \\nwe used bangkok airways twice during our tri...\n",
       "73267     \\n100  we have done this trip for last 6 years...\n",
       "73924     \\neverything was fine   neg_fnb was as expecte...\n",
       "83017     \\ncopa connects area through panama and you ca...\n",
       "83295     \\nthis is my national airline so i might neg_b...\n",
       "                                ...                        \n",
       "194570    \\ni have recently flown twice to adelaide and ...\n",
       "194703    \\nhave flown with them on number of occasions ...\n",
       "195836    \\nonly local airline i will fly with   my fami...\n",
       "196957    \\nflew this airline multiple times and have to...\n",
       "197680    \\nthis was short flight  but everything was fi...\n",
       "203534    \\ni flew to new york via shannon with my 2 dau...\n",
       "212970    \\nvacation travel with air india express is ad...\n",
       "213079     flight was fine and even left little earlier ...\n",
       "218019    \\nhas been flying with air asia for quite some...\n",
       "222193    \\nimprovements are visible and services is gro...\n",
       "222483     flight was i inexpensive enough for oneway fl...\n",
       "232581    \\nas we were last 2 to check in at male  we we...\n",
       "234478     flight was ok but assistants were neg_helpful...\n",
       "234712    \\nryanair is all about making money  they don ...\n",
       "238833    \\nit was same as all others company s only dif...\n",
       "239064    \\nevery little thing cost extra money that add...\n",
       "239459    \\nadd money for all extras   and keep your wal...\n",
       "240219    \\nthis airline is known for cancelations delay...\n",
       "240249     united flight was exactly what you want from ...\n",
       "244813    \\ni ve flown with them few times it isn t low ...\n",
       "244917    \\nplane is actually neg_bad consider its just ...\n",
       "248193    \\none of most expensive airflight company to u...\n",
       "251707    \\nfor national airline    it is embarrassment ...\n",
       "252736    \\njust arrived from beijing in ca845 flight  t...\n",
       "253304    \\nair france promotes extra seat for over size...\n",
       "255320    \\nａｆｔｅｒ　ｓｅｒｖｉｃｉｎｇ　ｔｈｅ　ｍｅａｌ 　ｈａｒｄ　ｔｏ　ｓｅｅ　ａｉｒ　ａｔ...\n",
       "255499    \\ni needed to fly from toronto to manchester a...\n",
       "256046    \\nwe have two delays of our flight because air...\n",
       "256479    \\nfound ants in flight  it was strange to see ...\n",
       "256576     seat is getting better compare when it start ...\n",
       "Name: preprocessed_text, Length: 78, dtype: object"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['preprocessed_text'][df['L1_prediction']==-1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M1 (Machine Learning- Naive Bayes Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_rating_b']= df['review_rating'].apply(lambda x: True if x >=40 else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "train_data = df[['preprocessed_text', 'review_rating_b']]\n",
    "train_data = shuffle(train_data)\n",
    "pipe = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),('clf', MultinomialNB()),])\n",
    "model = pipe.fit(train_data['preprocessed_text'],train_data['review_rating_b'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Prediction from M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['M1_prediction'] = model.predict(df['preprocessed_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A part of our final train dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    index  id  review_rating split  SentiWordnetH  \\\n",
      "1     754   2             10  test       0.000090   \n",
      "2     755   2             20  test       0.066018   \n",
      "3     756   2             30  test       0.003976   \n",
      "4     757   2             20  test       0.033217   \n",
      "5     758   2             30  test       0.049401   \n",
      "6     759   2             10  test       0.010155   \n",
      "7     760   2             10  test      -0.057323   \n",
      "8     761   2             10  test      -0.011332   \n",
      "9     762   2             10  test       0.033313   \n",
      "10    763   2             30  test       0.110153   \n",
      "\n",
      "                                    preprocessed_text  L1_prediction  \\\n",
      "1   \\nthis year was same as last  always late  tv ...       0.003314   \n",
      "2   \\nyour typical round trip flight to europe min...       0.392721   \n",
      "3   \\nflying back from ireland was only little bet...       0.036385   \n",
      "4   \\nwhen we checked in flight was shown as depar...      -0.183497   \n",
      "5   \\ntight squeeze walking to back of plane and i...       0.112167   \n",
      "6    woman who checked us in has attitude and make...       0.027688   \n",
      "7   \\nthis has been worst trip with american  both...      -0.251508   \n",
      "8   \\nwe have had major issues on last 3 flights w...      -0.140887   \n",
      "9   \\ni wanted to leave early so i was told by pho...      -0.089240   \n",
      "10  \\nthis flight was fine  it left on time and la...       0.139991   \n",
      "\n",
      "    M1_prediction  \n",
      "1           False  \n",
      "2            True  \n",
      "3           False  \n",
      "4           False  \n",
      "5            True  \n",
      "6           False  \n",
      "7           False  \n",
      "8           False  \n",
      "9           False  \n",
      "10          False  \n"
     ]
    }
   ],
   "source": [
    "print(df.loc[1:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the resulting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql('test_labelled', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
